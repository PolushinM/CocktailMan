{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c0b3fc",
   "metadata": {},
   "source": [
    "## Classifier: dynamic balance + albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbac7c",
   "metadata": {},
   "source": [
    "В этом ноутбуке производится обучение классификатора, который по изображению коктейля предсказывает набор ингредиентов. Использованы семплер для динамической балансировки классов из библиотеки Catalyst и аугментации из библиотеки albumentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cdb95b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "from math import log\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openvino.runtime import Core\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "from torch.optim import lr_scheduler\n",
    "import timm\n",
    "from catalyst.data.sampler import DynamicBalanceClassSampler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='darkgrid', font_scale=1.2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ed30e",
   "metadata": {},
   "source": [
    "### Подготовка данных для обучения:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a27e38",
   "metadata": {},
   "source": [
    "Получаем список коктейлей (наименование коктейля соответствует наименованию каталога)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "062d21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/maksim/Cocktails/Images/Coctails_raw/'\n",
    "files = sorted(list(Path(DATA_DIR).rglob('*.*')))\n",
    "full_labels = [path.parent.name for path in files]\n",
    "cockt_list = sorted(set(full_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd4049",
   "metadata": {},
   "source": [
    "Читаем из конфига (json заполняется руками) наименования ингредиентов на двух языках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f488a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening ingredients JSON config\n",
    "with open('config/ingredients.json', 'r') as f:\n",
    "    ingedients_config = json.load(f)\n",
    "\n",
    "class_labels = ingedients_config[\"idx\"]\n",
    "id2rus_genitive = ingedients_config[\"id2rus_genitive\"]\n",
    "class_labels_ru = np.array([id2rus_genitive[idx] for idx in class_labels])\n",
    "\n",
    "class_dict = dict()\n",
    "for i in range(len(class_labels)):\n",
    "    class_dict[class_labels[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525fac1",
   "metadata": {},
   "source": [
    "Проверяем, что русскоязычные наименования соответствуют идентификаторам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, rus in zip(class_labels, class_labels_ru):\n",
    "    print(idx, ' = ', rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaced19",
   "metadata": {},
   "source": [
    "Читаем из конфига (json заполняется руками) рецепты коктейлей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "178cefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening recipes JSON config\n",
    "with open('config/recipes.json', 'r') as f:\n",
    "    text_recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc79db",
   "metadata": {},
   "source": [
    "Проверяем, что перечень ингредиентов в двух конфигах совпадает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9bbeb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Количество ингредиентов: 45\n"
     ]
    }
   ],
   "source": [
    "ing_set = set()\n",
    "for rec in text_recipes:\n",
    "    ing_set.update(text_recipes[rec])\n",
    "    \n",
    "print(all(a == b for a, b in zip(sorted(ing_set), sorted(class_labels))))\n",
    "print(len(ing_set) == len(class_labels))\n",
    "\n",
    "for a, b in zip(sorted(ing_set), sorted(class_labels)):\n",
    "    if a != b:\n",
    "        print(a, b)\n",
    "print(f'Количество ингредиентов: {len(ing_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9e25f",
   "metadata": {},
   "source": [
    "Проверяем, что список коктейлей в каталоге на диске совпадает со списком в конфиге:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eae07a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Количество напитков: 153\n"
     ]
    }
   ],
   "source": [
    "print(all(folder == conf for folder, conf in zip(cockt_list, text_recipes.keys())))\n",
    "print(len(cockt_list) == len(text_recipes))\n",
    "\n",
    "for folder, conf in zip(cockt_list, text_recipes.keys()):\n",
    "    if folder!=conf:\n",
    "        print(folder, conf)\n",
    "print(f'Количество напитков: {len(cockt_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e0d4c",
   "metadata": {},
   "source": [
    "Формируем векторные представления рецептов (0 - ингредиент отсутствует в коктейле, 1 - ингредиент входит в состав)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a8bb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = dict()\n",
    "for cocktail, recipe in text_recipes.items():\n",
    "    arr = torch.zeros(len(class_labels), dtype=torch.int)\n",
    "    arr[[class_dict[ingr] for ingr in recipe]] = 1\n",
    "    recipes[cocktail] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24da21b",
   "metadata": {},
   "source": [
    "Задаём размер изображения, каталог с изображениями (убучающей выборкой) и константы нормализации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7795c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening model JSON config\n",
    "with open('config/model_classifier.json', 'r') as f:\n",
    "    model_conf = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7662478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imge size = 160x160\n",
      "Crop size = 180x180\n"
     ]
    }
   ],
   "source": [
    "image_size = model_conf['IMAGE_SIZE']\n",
    "crop_size = model_conf['CROP_SIZE']\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "print(f'Imge size = {image_size}x{image_size}')\n",
    "print(f'Crop size = {crop_size}x{crop_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "181a0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generated_images(generated):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(24, 8))\n",
    "    for k in range(len(generated)):\n",
    "        plt.subplot(1, len(generated), k+1)\n",
    "        plt.imshow(denorm(np.rollaxis(generated[k].numpy(), 0, 3)))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def denorm(img_tensors):\n",
    "    return img_tensors * stats[1][0] + stats[0][0]\n",
    "\n",
    "def show_images(images, nmax=200):\n",
    "    fig, ax = plt.subplots(figsize=(60, 60))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "\n",
    "def show_batch(dl, nmax=64):\n",
    "    for images, _ in dl:\n",
    "        show_images(images, nmax)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0ea81",
   "metadata": {},
   "source": [
    "#### Датасет и даталоадер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b225a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разные режимы датасета \n",
    "DATA_MODES = ['train', 'val', 'test']\n",
    "\n",
    "class CocktailsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, files, recipes: dict[str: np.array], mode, transforms):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        # список файлов для загрузки\n",
    "        self.files = files\n",
    "        # режим работы\n",
    "        self.mode = mode\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "            \n",
    "        str_labels = [path.parent.name for path in files]\n",
    "        self.labels = [recipes[label] for label in str_labels]    \n",
    "            \n",
    "        return None\n",
    "                      \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        image = Image.open(file, formats=[\"JPEG\", \"PNG\", \"GIF\", \"WEBP\"])\n",
    "        image.draft('RGB', (crop_size*2, crop_size*2)) \n",
    "        return np.array(image.convert(\"RGB\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.load_sample(self.files[index])\n",
    "        x = self.transforms(image=x)['image']\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            y = self.labels[index]\n",
    "            return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d556ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(list(Path(DATA_DIR).rglob('*.*')))\n",
    "\n",
    "full_labels = [path.parent.name for path in files]\n",
    "train_files, val_files = train_test_split(files, test_size=600, \\\n",
    "                                          stratify=full_labels)\n",
    "train_labels = [path.parent.name for path in train_files]\n",
    "\n",
    "transforms=A.Compose([A.SmallestMaxSize(max_size=crop_size*2),\n",
    "                   A.CenterCrop(crop_size*2, crop_size*2),\n",
    "                   A.RandomResizedCrop(image_size, \n",
    "                                       image_size, \n",
    "                                       scale=(image_size/crop_size, 1.0), \n",
    "                                       ratio=(0.85, 1.176)\n",
    "                                      ),\n",
    "                   A.HorizontalFlip(p=0.5),\n",
    "                   A.ColorJitter(brightness=(0.96, 1.0),\n",
    "                                 contrast=(0.96, 1.0),\n",
    "                                 saturation=(0.93, 1.03), \n",
    "                                 hue=0.015, \n",
    "                                 p=0.7),\n",
    "                   A.RandomToneCurve(scale=0.07, p=0.7),\n",
    "                   A.Normalize(*stats),\n",
    "                   ToTensorV2() ])\n",
    "\n",
    "train_dataset = CocktailsDataset(train_files, \n",
    "                                 mode='train', \n",
    "                                 recipes=recipes,\n",
    "                                 transforms=transforms)\n",
    "\n",
    "val_dataset = CocktailsDataset(val_files, \n",
    "                               mode='val', \n",
    "                               recipes=recipes, \n",
    "                               transforms=A.Compose([A.SmallestMaxSize(max_size=crop_size),\n",
    "                                                     A.CenterCrop(image_size, image_size),\n",
    "                                                     A.Normalize(*stats),\n",
    "                                                     ToTensorV2() ]) )\n",
    "\n",
    "full_dataset = CocktailsDataset(files, \n",
    "                                mode='train', \n",
    "                                recipes=recipes,\n",
    "                                transforms=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb53ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for batch in self.dataloader: \n",
    "            yield to_device(batch, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    \n",
    "def get_dataloaders(image_size, batch_size, train_set, test_set, sampler=None):\n",
    "    shuffle = sampler is None\n",
    "    train_dataloader = DataLoader(train_set, batch_size, shuffle=shuffle, sampler=sampler, num_workers=6, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_set, batch_size, shuffle=True, num_workers=6, pin_memory=True)\n",
    "    return DeviceDataLoader(train_dataloader, device), DeviceDataLoader(test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda5a61",
   "metadata": {},
   "source": [
    "Взглянем на аугментированные изображения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec43e2c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "train_loader, val_loader = get_dataloaders(image_size, \n",
    "                                           batch_size, \n",
    "                                           train_set=train_dataset, \n",
    "                                           test_set=val_dataset, \n",
    "                                           sampler=None)\n",
    "\n",
    "first_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c0551",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(first_batch[0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ef59f",
   "metadata": {},
   "source": [
    "### Обучение нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77f0e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer, sheduler, threshold, label_smoothing):\n",
    "    running_loss = 0.0\n",
    "    running_recall = 0\n",
    "    running_precision = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels * (1 - label_smoothing) + label_smoothing / 20\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = (outputs > -log(1 / threshold - 0.999)) * 1\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_recall += (torch.sum(preds.data * labels.data) + 0.1) / (torch.sum(labels.data) + 0.1) * inputs.size(0)\n",
    "        running_precision += (torch.sum(preds.data * labels.data) + 0.1) / (torch.sum(preds.data) + 0.1) * inputs.size(0)\n",
    "        processed_data += inputs.size(0)\n",
    "    sheduler.step()\n",
    "    train_loss = running_loss / processed_data\n",
    "    recall = running_recall.double() / processed_data\n",
    "    precision = running_precision.double() / processed_data\n",
    "    train_acc = (2*recall*precision) / (precision+recall) \n",
    "    return train_loss, train_acc\n",
    "\n",
    "def eval_epoch(model, val_loader, criterion, threshold, label_smoothing):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_recall = 0\n",
    "    running_precision = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels * (1 - label_smoothing) + label_smoothing / 20\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = (outputs > -log(1 / threshold - 0.999)) * 1\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_recall += (torch.sum(preds.data * labels.data) + 0.1) / (torch.sum(labels.data) + 0.1) * inputs.size(0)\n",
    "        running_precision += (torch.sum(preds.data * labels.data) + 0.1) / (torch.sum(preds.data) + 0.1) * inputs.size(0)\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    recall = running_recall.double() / processed_size\n",
    "    precision = running_precision.double() / processed_size\n",
    "    val_acc = (2*recall*precision) / (precision+recall) \n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train(train_loader, val_loader, model, epochs, optimizer, gamma=0.95, threshold=0.5, label_smoothing=0.):\n",
    "\n",
    "    model.to(device)\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        sheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, \n",
    "                                              train_loader, \n",
    "                                              criterion, \n",
    "                                              optimizer, \n",
    "                                              sheduler, \n",
    "                                              threshold, \n",
    "                                              label_smoothing)\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion, threshold, label_smoothing)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec437d2c",
   "metadata": {},
   "source": [
    "В качестве нейронной сети для обучения выбрана предобученная MobilenetV3. \\\n",
    "Немного меняем структуру сети: \n",
    "1. Послендий полносвязный слой заменяем двумя слоями, bottleneck размерности 14 и финальный классификатор с размерностью, равной количеству ингредиентов.\n",
    "2. Немного уменьшаем размерность 6-го блока для того, чтобы снизить сложность сети и увеличить её стойкость к переобучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cde1af8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = timm.create_model('mobilenetv3_large_100_miil', pretrained=True).to(device) \n",
    "\n",
    "model.blocks[6][0].conv = nn.Conv2d(160, 256, 1, 1).to(device)\n",
    "model.blocks[6][0].bn1 = nn.BatchNorm2d(256).to(device)\n",
    "model.conv_head = nn.Sequential(nn.Conv2d(256, 600, 1, 1) ).to(device)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.75),\n",
    "    nn.Linear(in_features=600, out_features=28, bias=False),\n",
    "    nn.BatchNorm1d(28),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.LeakyReLU(0.4),\n",
    "    nn.Linear(in_features=28, out_features=len(class_labels))).to(device)\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571592d",
   "metadata": {},
   "source": [
    "Поучившаяся структура модели:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de3371",
   "metadata": {},
   "source": [
    "Размораживаем часть градиентов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.blocks[1].requires_grad_(True)\n",
    "model.blocks[2].requires_grad_(True)\n",
    "model.blocks[3].requires_grad_(True)\n",
    "model.blocks[4].requires_grad_(True)\n",
    "model.blocks[5].requires_grad_(True)\n",
    "model.blocks[6].requires_grad_(True)\n",
    "\n",
    "model.conv_head.requires_grad_(True)\n",
    "model.classifier.requires_grad_(True)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab435b",
   "metadata": {},
   "source": [
    "Для того чтобы максимально сохранить информацию в предобученной сети, устанавливаем разные скорости обучения для различных групп слоёв: чем ближе в выходу сети, тем выше скорость обучения. Вводим новый гиперпараметр lr_decay, который определяет, насколько будут отличаться веса на разных уровнях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0329e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(lr: float, lr_decay: float) -> object:\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.conv_stem.parameters()},\n",
    "        {'params': model.bn1.parameters()},\n",
    "        {'params': model.blocks[0].parameters(), 'lr': lr/lr_decay**7},\n",
    "        {'params': model.blocks[1].parameters(), 'lr': lr/lr_decay**6},\n",
    "        {'params': model.blocks[2].parameters(), 'lr': lr/lr_decay**5},\n",
    "        {'params': model.blocks[3].parameters(), 'lr': lr/lr_decay**4},\n",
    "        {'params': model.blocks[4].parameters(), 'lr': lr/lr_decay**3},\n",
    "        {'params': model.blocks[5].parameters(), 'lr': lr/lr_decay**2},\n",
    "        {'params': model.blocks[6].parameters(), 'lr': lr/lr_decay**1},\n",
    "        {'params': model.conv_head.parameters(), 'lr': lr/lr_decay**1},\n",
    "        {'params': model.classifier.parameters(), 'lr': lr},\n",
    "    ], lr=lr/lr_decay**8)\n",
    "    return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb0456",
   "metadata": {},
   "source": [
    "Обучаем сеть в несколько циклов с различными значениями lr, lr_decay, batch_size (сначала обучаем \"голову\" сети, потом учим \"в глубину\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ebc040a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the smallest class contains only 9 examples. At the end of training, epochs will contain only 1377 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe2d50e09aa4413b7029b8822f58d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.26750840482825844\n",
      "\n",
      "Epoch 001 train_loss: 0.2675     val_loss 0.1671 train_acc 0.3540 val_acc 0.5193\n",
      "loss 0.14658140365470498\n",
      "\n",
      "Epoch 002 train_loss: 0.1466     val_loss 0.1299 train_acc 0.6103 val_acc 0.6500\n",
      "loss 0.12133446454513232\n",
      "\n",
      "Epoch 003 train_loss: 0.1213     val_loss 0.1109 train_acc 0.6965 val_acc 0.7211\n",
      "loss 0.10687974665650572\n",
      "\n",
      "Epoch 004 train_loss: 0.1069     val_loss 0.0998 train_acc 0.7396 val_acc 0.7600\n",
      "loss 0.09691511203343729\n",
      "\n",
      "Epoch 005 train_loss: 0.0969     val_loss 0.0874 train_acc 0.7697 val_acc 0.7986\n",
      "loss 0.0897391755313414\n",
      "\n",
      "Epoch 006 train_loss: 0.0897     val_loss 0.0814 train_acc 0.7889 val_acc 0.8121\n"
     ]
    }
   ],
   "source": [
    "lr = 7.0e-3\n",
    "lr_decay = 6.0\n",
    "batch_size = 200\n",
    "l_sm=0.01\n",
    "sampler = DynamicBalanceClassSampler(labels=full_labels, exp_lambda=0.98)\n",
    "\n",
    "train_loader, val_loader = get_dataloaders(image_size, batch_size, full_dataset, val_dataset, sampler=sampler)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history1 = train(train_loader, val_loader, model, epochs=6, optimizer=optimizer, gamma=0.85, label_smoothing=l_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ed28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 4e-3\n",
    "lr_decay = 2.5\n",
    "batch_size = 80\n",
    "l_sm=0.03\n",
    "sampler = DynamicBalanceClassSampler(labels=full_labels, exp_lambda=0.98)\n",
    "\n",
    "train_loader, val_loader = get_dataloaders(image_size, batch_size, full_dataset, val_dataset, sampler=sampler)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history2 = train(train_loader, val_loader, model, epochs=5, optimizer=optimizer, gamma=0.85, label_smoothing=l_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61358a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1.3e-3\n",
    "lr_decay = 1.3\n",
    "batch_size = 80\n",
    "l_sm=0.03\n",
    "sampler = DynamicBalanceClassSampler(labels=full_labels, exp_lambda=0.97)\n",
    "\n",
    "train_loader, val_loader = get_dataloaders(image_size, batch_size, full_dataset, val_dataset, sampler=sampler)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history3 = train(train_loader, val_loader, model, epochs=24, optimizer=optimizer, gamma=0.9, label_smoothing=l_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b88f4",
   "metadata": {},
   "source": [
    "Сохраняем модель, веса. При необходимости, загружаем веса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb28667",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'classifier_state_dict_ls.pt')\n",
    "torch.save(model, 'classifier_model_ls.pt')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a86fa86f",
   "metadata": {},
   "source": [
    "model.load_state_dict(torch.load('classifier_state_dict.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a4629",
   "metadata": {},
   "source": [
    "Проверим работу модели на случайном изображении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15515bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ingredients(path: str, model: callable, classes: np.array) -> list:\n",
    "    try:\n",
    "        image = Image.open(path)\n",
    "    except:\n",
    "        return []\n",
    "    width, height = image.size  # Get dimensions\n",
    "    size = min(width, height)\n",
    "    \n",
    "    left = (width - size) / 2\n",
    "    top = (height - size) / 2\n",
    "    right = (width + size) / 2\n",
    "    bottom = (height + size) / 2\n",
    "\n",
    "    # Crop the center of the image\n",
    "    image = image.crop((left, top, right, bottom))\n",
    "    img = np.asarray(image.resize((image_size, image_size))) / 127.5 - 1.0\n",
    "    \n",
    "    plt.figure(figsize=(2.5, 2.5))\n",
    "    plt.imshow(denorm(img))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    logits = model(torch.tensor(np.rollaxis(img, 2, 0)[None, :, :, :], dtype=torch.float).to(device))\n",
    "    result = (logits.detach().cpu() > 0).nonzero()[:, 1].numpy()\n",
    "    return classes[result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45781db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ingredients('/home/maksim/Cocktails/Images/Coctails_raw/Bloody_mary/AD4A0735.jpg', model, class_labels_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba4ccd",
   "metadata": {},
   "source": [
    "### Конвертация в ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb0350",
   "metadata": {},
   "source": [
    "Загружаем модель в pytorch и экспортируем в формат ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf004b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.load_state_dict(torch.load('classifier_state_dict_ls.pt'))\n",
    "# Evaluate the model to switch some operations from training mode to inference.\n",
    "model.eval()\n",
    "# Create dummy input for the model. It will be used to run the model inside export function.\n",
    "dummy_input = torch.randn(1, 3, image_size, image_size)\n",
    "# Call the export function\n",
    "torch.onnx.export(model,               \n",
    "                  dummy_input,                         \n",
    "                  'classifier_ls.onnx',   \n",
    "                  export_params=True,        \n",
    "                  opset_version=11,          \n",
    "                  do_constant_folding=True,  \n",
    "                  input_names = ['input'],   \n",
    "                  output_names = ['logits']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ebcb41",
   "metadata": {},
   "source": [
    "Загружаем модель ONNX для проверки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e12ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8c3d6",
   "metadata": {},
   "source": [
    "Доступные для ONNX устройства:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aec88642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: AMD FX(tm)-6300 Six-Core Processor             \n"
     ]
    }
   ],
   "source": [
    "devices = ie.available_devices\n",
    "\n",
    "for dev in devices:\n",
    "    device_name = ie.get_property(device_name=dev, name=\"FULL_DEVICE_NAME\")\n",
    "    print(f\"{dev}: {device_name}\")\n",
    "    \n",
    "onnx_model_path = 'classifier_ls.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656fe479",
   "metadata": {},
   "source": [
    "Вход модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae9eba83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input precision: <Type: 'float32'>\n",
      "input shape: {1, 3, 160, 160}\n"
     ]
    }
   ],
   "source": [
    "input_layer = compiled_model_onnx.input(0)\n",
    "\n",
    "print(f\"input precision: {input_layer.element_type}\")\n",
    "print(f\"input shape: {input_layer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c31e4",
   "metadata": {},
   "source": [
    "Выход модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa591d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output precision: <Type: 'float32'>\n",
      "output shape: {1, 45}\n"
     ]
    }
   ],
   "source": [
    "output_layer = compiled_model_onnx.output(0)\n",
    "\n",
    "print(f\"output precision: {output_layer.element_type}\")\n",
    "print(f\"output shape: {output_layer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49d229",
   "metadata": {},
   "source": [
    "Вся загрузка в одной ячейке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fa768aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "onnx_model_path = 'classifier_ls.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "\n",
    "input_layer = compiled_model_onnx.input(0)\n",
    "output_layer = compiled_model_onnx.output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce24e2",
   "metadata": {},
   "source": [
    "Пробуем инференс на случайном изображении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80b1380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ingredients_onnx(path: str, class_labels: list) -> list:\n",
    "    img = np.asarray(Image.open(path).resize((image_size, image_size))) / 255\n",
    "    logits = compiled_model_onnx([np.rollaxis(img, 2, 0)[None, :, :, :]])[output_layer]\n",
    "    result = (logits > 0.5).nonzero()[1]\n",
    "    return class_labels_ru[result]\n",
    "\n",
    "def generate_recipe(ingredients: list) -> str:\n",
    "    return ', '.join(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0eaec2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "спрайт, ром, сахарный сироп, лайм, мяту, лёд\n"
     ]
    }
   ],
   "source": [
    "img_path = '/home/maksim/Cocktails/Images/Coctails_raw/Mojito/0346a20835_1000.jpg'\n",
    "print(generate_recipe(predict_ingredients_onnx(img_path, class_labels=class_labels_ru)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9a8cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_vector(path: str, class_labels: list) -> list:\n",
    "    img = np.asarray(Image.open(path).resize((image_size, image_size))) / 255\n",
    "    logits = compiled_model_onnx([np.rollaxis(img, 2, 0)[None, :, :, :]])[output_layer][0]\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    pos_ind = (probs > 0.5).nonzero()[0]\n",
    "    neg_ind = (probs < 0.5).nonzero()[0]\n",
    "    \n",
    "    return np.prod(probs[pos_ind])*np.prod(1-probs[neg_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4db95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
