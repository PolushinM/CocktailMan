{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c0b3fc",
   "metadata": {},
   "source": [
    "## Classifier Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbac7c",
   "metadata": {},
   "source": [
    "В этом ноутбуке производится обучение классификатора, который по изображению коктейля предсказывает набор ингредиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb95b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "from math import log\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openvino.runtime import Core\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "from torch.optim import lr_scheduler\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='darkgrid', font_scale=1.2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ed30e",
   "metadata": {},
   "source": [
    "### Подготовка данных для обучения:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a27e38",
   "metadata": {},
   "source": [
    "Получаем список коктейлей (наименование коктейля соответствует наименованию каталога)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cockt_list = glob('/home/maksim/Cocktails/Images/Coctails_raw/*', )\n",
    "cockt_list = sorted([x[43:] for x in cockt_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd4049",
   "metadata": {},
   "source": [
    "Читаем из конфига (json заполняется руками) наименования ингредиентов на двух языках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening ingredients JSON config\n",
    "with open('config/ingredients.json', 'r') as f:\n",
    "    ingedients_config = json.load(f)\n",
    "\n",
    "class_labels = ingedients_config[\"idx\"]\n",
    "id2rus_genitive = ingedients_config[\"id2rus_genitive\"]\n",
    "class_labels_ru = np.array([id2rus_genitive[idx] for idx in class_labels])\n",
    "\n",
    "class_dict = dict()\n",
    "for i in range(len(class_labels)):\n",
    "    class_dict[class_labels[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525fac1",
   "metadata": {},
   "source": [
    "Проверяем, что русскоязычные наименования соответствуют идентификаторам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85decb44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, rus in zip(class_labels, class_labels_ru):\n",
    "    print(idx, ' = ', rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaced19",
   "metadata": {},
   "source": [
    "Читаем из конфига (json заполняется руками) рецепты коктейлей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening recipes JSON config\n",
    "with open('config/recipes.json', 'r') as f:\n",
    "    text_recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc79db",
   "metadata": {},
   "source": [
    "Проверяем, что перечень ингредиентов в двух конфигах совпадает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bbeb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ing_set = set()\n",
    "for rec in text_recipes:\n",
    "    ing_set.update(text_recipes[rec])\n",
    "    \n",
    "print(all(a == b for a, b in zip(sorted(ing_set), sorted(class_labels))))\n",
    "print(len(ing_set) == len(class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9e25f",
   "metadata": {},
   "source": [
    "Проверяем, что список коктейлей в каталоге совпадает со списком в конфиге:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae07a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(all(folder == conf for folder, conf in zip(cockt_list, text_recipes.keys())))\n",
    "print(len(cockt_list) == len(text_recipes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e0d4c",
   "metadata": {},
   "source": [
    "Формируем векторные представления рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = dict()\n",
    "for cocktail, recipe in text_recipes.items():\n",
    "    arr = torch.zeros(len(class_labels), dtype=torch.int)\n",
    "    arr[[class_dict[ingr] for ingr in recipe]] = 1\n",
    "    recipes[cocktail] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24da21b",
   "metadata": {},
   "source": [
    "Задаём размер изображения, каталог с изображениями (убучающей выборкой) и константы нормализации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening model JSON config\n",
    "with open('config/model.json', 'r') as f:\n",
    "    model_conf = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7662478",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = model_conf['image_size']\n",
    "crop_size = model_conf['crop_size']\n",
    "DATA_DIR = '/home/maksim/Cocktails/Images/Coctails_raw/'\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "print(f'Imge size = {image_size}x{image_size}')\n",
    "print(f'Crop size = {crop_size}x{crop_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generated_images(generated):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(24, 8))\n",
    "    for k in range(len(generated)):\n",
    "        plt.subplot(1, len(generated), k+1)\n",
    "        plt.imshow(denorm(np.rollaxis(generated[k].numpy(), 0, 3)))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def denorm(img_tensors):\n",
    "    return img_tensors * stats[1][0] + stats[0][0]\n",
    "\n",
    "def show_images(images, nmax=200):\n",
    "    fig, ax = plt.subplots(figsize=(60, 60))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "\n",
    "def show_batch(dl, nmax=64):\n",
    "    for images, _ in dl:\n",
    "        show_images(images, nmax)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8284ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = 0\n",
    "for cockt in recipes:\n",
    "    summ += recipes[cockt].sum()\n",
    "summ = summ / recipes[next(iter(recipes))].shape[0] / len(recipes)\n",
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50bcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556ce1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for batch in self.dataloader: \n",
    "            yield to_device(batch, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    \n",
    "def get_dataloaders(image_size, batch_size):\n",
    "    \"\"\"\n",
    "    Builds dataloader for training data.\n",
    "    Use tt.Compose and tt.Resize for transformations\n",
    "    :param image_size: height and wdith of the image\n",
    "    :param batch_size: batch_size of the dataloader\n",
    "    :returns: DeviceDataLoader object \n",
    "    \"\"\"\n",
    "  \n",
    "    full_set = ImageFolder(DATA_DIR, \n",
    "                           transform=tt.Compose([\n",
    "                                  tt.Resize(crop_size),\n",
    "                                  tt.CenterCrop(crop_size),\n",
    "                                  tt.ColorJitter(brightness=(0.8, 1.1), \n",
    "                                                 contrast=(0.8, 1.1), \n",
    "                                                 saturation=(0.8, 1.1), \n",
    "                                                 hue=0.015),\n",
    "                                  tt.ToTensor(),\n",
    "                                  tt.Normalize(*stats),\n",
    "                                  tt.RandomCrop(image_size),\n",
    "                                  tt.RandomHorizontalFlip() ])\n",
    "                               )\n",
    "    \n",
    "    recipe_labels = torch.zeros((len(full_set.classes), len(class_labels)))\n",
    "    for cockt, idx in full_set.class_to_idx.items():\n",
    "        recipe_labels[idx] = recipes[cockt]\n",
    "    \n",
    "    \n",
    "    n = len(full_set)  \n",
    "    n_test = int(0.032 * n)  \n",
    "    permutation = list(range(n))\n",
    "    random.Random(42).shuffle(permutation)\n",
    "    train_set = torch.utils.data.Subset(full_set, permutation[n_test: n])  \n",
    "    test_set = torch.utils.data.Subset(full_set, permutation[:n_test])  \n",
    "    \n",
    "    train_dataloader = DataLoader(train_set, batch_size, shuffle=True, num_workers=5, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_set, batch_size, shuffle=True, num_workers=5, pin_memory=True)\n",
    "\n",
    "    return DeviceDataLoader(train_dataloader, device), DeviceDataLoader(test_dataloader, device), recipe_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda5a61",
   "metadata": {},
   "source": [
    "Взглянем на аугментированные изображения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43e2c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "train_loader, val_loader, recipe_labels = get_dataloaders(image_size, batch_size)\n",
    "show_images(next(iter(val_loader))[0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ef59f",
   "metadata": {},
   "source": [
    "### Обучение нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer, sheduler, threshold, label_smoothing):\n",
    "    running_loss = 0.0\n",
    "    running_recall = 0\n",
    "    running_precision = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels_idx in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = recipe_labels[labels_idx] * (1 - label_smoothing) + label_smoothing / 20\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = (outputs > -log(1 / threshold - 0.999)) * 1\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_recall += (torch.sum(preds.data * labels.data) + 0.1) / (torch.sum(labels.data) + 0.1) * inputs.size(0)\n",
    "        running_precision += (torch.sum(preds.data * labels.data) + 0.1) / (torch.sum(preds.data) + 0.1) * inputs.size(0)\n",
    "        processed_data += inputs.size(0)\n",
    "    sheduler.step()\n",
    "    train_loss = running_loss / processed_data\n",
    "    recall = running_recall.double() / processed_data\n",
    "    precision = running_precision.double() / processed_data\n",
    "    train_acc = (2*recall*precision) / (precision+recall) \n",
    "    return train_loss, train_acc\n",
    "\n",
    "def eval_epoch(model, val_loader, criterion, threshold, label_smoothing):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_recall = 0\n",
    "    running_precision = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels_idx in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = recipe_labels[labels_idx] * (1 - label_smoothing) + label_smoothing / 20\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = (outputs > -log(1 / threshold - 0.999)) * 1\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_recall += (torch.sum(preds.data * labels.data) + 0.1) / (torch.sum(labels.data) + 0.1) * inputs.size(0)\n",
    "        running_precision += (torch.sum(preds.data * labels.data) + 0.1) / (torch.sum(preds.data) + 0.1) * inputs.size(0)\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    recall = running_recall.double() / processed_size\n",
    "    precision = running_precision.double() / processed_size\n",
    "    val_acc = (2*recall*precision) / (precision+recall) \n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train(train_loader, val_loader, model, epochs, optimizer, gamma=0.95, threshold=0.5, label_smoothing=0.):\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        sheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, \n",
    "                                              train_loader, \n",
    "                                              criterion, \n",
    "                                              optimizer, \n",
    "                                              sheduler, \n",
    "                                              threshold, \n",
    "                                              label_smoothing)\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion, threshold, label_smoothing)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec437d2c",
   "metadata": {},
   "source": [
    "В качестве нейронной сети для обучения выбрана предобученная MobilenetV3. \\\n",
    "Немного меняем структуру сети: \n",
    "1. Послендий полносвязный слой заменяем двумя слоями, bottleneck размерности 14 и финальный классификатор с размерностью, равной количеству ингредиентов.\n",
    "2. Немного уменьшаем размерность 6-го блока для того, чтобы снизить сложность сети и увеличить её стойкость к переобучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1af8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = timm.create_model('mobilenetv3_large_100_miil', pretrained=True).to(device) \n",
    "\n",
    "model.blocks[6][0].conv = nn.Conv2d(160, 300, 1, 1).to(device)\n",
    "model.blocks[6][0].bn1 = nn.BatchNorm2d(300).to(device)\n",
    "model.conv_head = nn.Conv2d(300, 700, 1, 1).to(device)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.75),\n",
    "    nn.Linear(in_features=700, out_features=30, bias=False),\n",
    "    nn.BatchNorm1d(30),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(in_features=30, out_features=len(class_labels))).to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571592d",
   "metadata": {},
   "source": [
    "Поучившаяся структура модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ffde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de3371",
   "metadata": {},
   "source": [
    "Размораживаем часть градиентов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae758f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.blocks[1].requires_grad_(True)\n",
    "model.blocks[2].requires_grad_(True)\n",
    "model.blocks[3].requires_grad_(True)\n",
    "model.blocks[4].requires_grad_(True)\n",
    "model.blocks[5].requires_grad_(True)\n",
    "model.blocks[6].requires_grad_(True)\n",
    "\n",
    "model.conv_head.requires_grad_(True)\n",
    "model.classifier.requires_grad_(True)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab435b",
   "metadata": {},
   "source": [
    "Для того чтобы максимально сохранить информацию в предобученной сети, устанавливаем разные скорости обучения для различных групп слоёв: чем ближе в выходу сети, тем выше скорость обучения. Вводим новый гиперпараметр lr_decay, который определяет, насколько будут отличаться веса на разных уровнях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(lr: float, lr_decay: float) -> object:\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.conv_stem.parameters()},\n",
    "        {'params': model.bn1.parameters()},\n",
    "        {'params': model.blocks[0].parameters(), 'lr': lr/lr_decay**7},\n",
    "        {'params': model.blocks[1].parameters(), 'lr': lr/lr_decay**6},\n",
    "        {'params': model.blocks[2].parameters(), 'lr': lr/lr_decay**5},\n",
    "        {'params': model.blocks[3].parameters(), 'lr': lr/lr_decay**4},\n",
    "        {'params': model.blocks[4].parameters(), 'lr': lr/lr_decay**3},\n",
    "        {'params': model.blocks[5].parameters(), 'lr': lr/lr_decay**2},\n",
    "        {'params': model.blocks[6].parameters(), 'lr': lr/lr_decay**1},\n",
    "        {'params': model.conv_head.parameters(), 'lr': lr/lr_decay**1},\n",
    "        {'params': model.classifier.parameters(), 'lr': lr},\n",
    "    ], lr=lr/lr_decay**8)\n",
    "    return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb0456",
   "metadata": {},
   "source": [
    "Обучаем сеть в несколько циклов с различными значениями lr, lr_decay, batch_size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc040a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 5.0e-3\n",
    "lr_decay = 6.0\n",
    "batch_size = 200\n",
    "l_sm=0.03\n",
    "\n",
    "train_loader, val_loader, recipe_labels = get_dataloaders(image_size, batch_size)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history1 = train(train_loader, val_loader, model, epochs=5, optimizer=optimizer, gamma=0.9, label_smoothing=l_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6deac3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 3.0e-3\n",
    "lr_decay = 2.0\n",
    "batch_size = 64\n",
    "l_sm=0.03\n",
    "\n",
    "train_loader, val_loader, recipe_labels = get_dataloaders(image_size, batch_size)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history4 = train(train_loader, val_loader, model, epochs=6, optimizer=optimizer, gamma=0.85, label_smoothing=l_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629f91b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 3.5e-3\n",
    "lr_decay = 2.2\n",
    "batch_size = 64\n",
    "l_sm=0.03\n",
    "\n",
    "train_loader, val_loader, recipe_labels = get_dataloaders(image_size, batch_size)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history3 = train(train_loader, val_loader, model, epochs=12, optimizer=optimizer, gamma=0.9, label_smoothing=l_sm)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f725b560",
   "metadata": {},
   "source": [
    "lr = 3.5e-3\n",
    "lr_decay = 2.5\n",
    "batch_size = 80\n",
    "l_sm=0.03\n",
    "\n",
    "train_loader, val_loader, recipe_labels = get_dataloaders(image_size, batch_size)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history2 = train(train_loader, val_loader, model, epochs=8, optimizer=optimizer, gamma=0.9, label_smoothing=l_sm)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64250317",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "lr = 3.5e-3 \n",
    "lr_decay = 2.8 \n",
    "batch_size = 200\n",
    "l_sm=0.02\n",
    "\n",
    "train_loader, val_loader, recipe_labels = get_dataloaders(image_size, batch_size)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history5 = train(val_loader, val_loader, model, epochs=12, optimizer=optimizer, gamma=0.9, label_smoothing=l_sm) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb7931b7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "lr = 3.5e-3\n",
    "lr_decay = 2.2\n",
    "batch_size = 64\n",
    "l_sm=0.02\n",
    "\n",
    "train_loader, val_loader, recipe_labels = get_dataloaders(image_size, batch_size)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history6 = train(train_loader, val_loader, model, epochs=4, optimizer=optimizer, gamma=0.85, label_smoothing=l_sm)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2165513",
   "metadata": {},
   "source": [
    "lr = 3.5e-3 \n",
    "lr_decay = 3.5 \n",
    "batch_size = 200\n",
    "l_sm=0.02\n",
    "\n",
    "train_loader, val_loader, recipe_labels = get_dataloaders(image_size, batch_size)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history6 = train(val_loader, val_loader, model, epochs=8, optimizer=optimizer, gamma=0.9, label_smoothing=l_sm) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f669003",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "lr = 1e-3 \n",
    "lr_decay = 2.5 \n",
    "batch_size = 200\n",
    "l_sm=0.02\n",
    "\n",
    "train_loader, val_loader, recipe_labels = get_dataloaders(image_size, batch_size)\n",
    "optimizer = get_optimizer(lr, lr_decay)\n",
    "\n",
    "history7 = train(train_loader, val_loader, model, epochs=1, optimizer=optimizer, gamma=0.5, label_smoothing=l_sm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b88f4",
   "metadata": {},
   "source": [
    "Сохраняем модель, веса. При необходимости, загружаем веса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb28667",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'classifier_state_dict_ls.pt')\n",
    "torch.save(model, 'classifier_model_ls.pt')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a86fa86f",
   "metadata": {},
   "source": [
    "model.load_state_dict(torch.load('classifier_state_dict.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a4629",
   "metadata": {},
   "source": [
    "Проверим работу модели на случайном изображении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15515bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ingredients(path: str, model: callable, classes: np.array) -> list:\n",
    "    try:\n",
    "        image = Image.open(path)\n",
    "    except:\n",
    "        return []\n",
    "    width, height = image.size  # Get dimensions\n",
    "    size = min(width, height)\n",
    "    \n",
    "    left = (width - size) / 2\n",
    "    top = (height - size) / 2\n",
    "    right = (width + size) / 2\n",
    "    bottom = (height + size) / 2\n",
    "\n",
    "    # Crop the center of the image\n",
    "    image = image.crop((left, top, right, bottom))\n",
    "    img = np.asarray(image.resize((image_size, image_size))) / 127.5 - 1.0\n",
    "    \n",
    "    plt.figure(figsize=(2.5, 2.5))\n",
    "    plt.imshow(denorm(img))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    logits = model(torch.tensor(np.rollaxis(img, 2, 0)[None, :, :, :], dtype=torch.float).to(device))\n",
    "    result = (logits.detach().cpu() > 0).nonzero()[:, 1].numpy()\n",
    "    return classes[result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f669f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_ingredients('/home/maksim/Cocktails/Images/Coctails_raw/Bloody_mary/AD4A0735.jpg', model, class_labels_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba4ccd",
   "metadata": {},
   "source": [
    "### Конвертация в ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb0350",
   "metadata": {},
   "source": [
    "Загружаем модель в pytorch и экспортируем в формат ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf004b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "model.load_state_dict(torch.load('classifier_state_dict_ls.pt'))\n",
    "# Evaluate the model to switch some operations from training mode to inference.\n",
    "model.eval()\n",
    "# Create dummy input for the model. It will be used to run the model inside export function.\n",
    "dummy_input = torch.randn(1, 3, image_size, image_size)\n",
    "# Call the export function\n",
    "torch.onnx.export(model,               \n",
    "                  dummy_input,                         \n",
    "                  'classifier_ls.onnx',   \n",
    "                  export_params=True,        \n",
    "                  opset_version=11,          \n",
    "                  do_constant_folding=True,  \n",
    "                  input_names = ['input'],   \n",
    "                  output_names = ['logits']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ebcb41",
   "metadata": {},
   "source": [
    "Загружаем модель ONNX для проверки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8c3d6",
   "metadata": {},
   "source": [
    "Доступные для ONNX устройства:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec88642",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = ie.available_devices\n",
    "\n",
    "for dev in devices:\n",
    "    device_name = ie.get_property(device_name=dev, name=\"FULL_DEVICE_NAME\")\n",
    "    print(f\"{dev}: {device_name}\")\n",
    "    \n",
    "onnx_model_path = 'classifier_ls.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656fe479",
   "metadata": {},
   "source": [
    "Вход модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9eba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = compiled_model_onnx.input(0)\n",
    "\n",
    "print(f\"input precision: {input_layer.element_type}\")\n",
    "print(f\"input shape: {input_layer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c31e4",
   "metadata": {},
   "source": [
    "Выход модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa591d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = compiled_model_onnx.output(0)\n",
    "\n",
    "print(f\"output precision: {output_layer.element_type}\")\n",
    "print(f\"output shape: {output_layer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49d229",
   "metadata": {},
   "source": [
    "Вся загрузка в одной ячейке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa768aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "onnx_model_path = 'classifier_ls.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "\n",
    "input_layer = compiled_model_onnx.input(0)\n",
    "output_layer = compiled_model_onnx.output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce24e2",
   "metadata": {},
   "source": [
    "Пробуем инференс на случайном изображении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ingredients_onnx(path: str, class_labels: list) -> list:\n",
    "    img = np.asarray(Image.open(path).resize((image_size, image_size))) / 255\n",
    "    logits = compiled_model_onnx([np.rollaxis(img, 2, 0)[None, :, :, :]])[output_layer]\n",
    "    result = (logits > 0.5).nonzero()[1]\n",
    "    return class_labels_ru[result]\n",
    "\n",
    "def generate_recipe(ingredients: list) -> str:\n",
    "    return ', '.join(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaec2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/maksim/Cocktails/Images/Coctails_raw/Mojito/0346a20835_1000.jpg'\n",
    "print(generate_recipe(predict_ingredients_onnx(img_path, class_labels=class_labels_ru)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_vector(path: str, class_labels: list) -> list:\n",
    "    img = np.asarray(Image.open(path).resize((image_size, image_size))) / 255\n",
    "    logits = compiled_model_onnx([np.rollaxis(img, 2, 0)[None, :, :, :]])[output_layer][0]\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    pos_ind = (probs > 0.5).nonzero()[0]\n",
    "    neg_ind = (probs < 0.5).nonzero()[0]\n",
    "    \n",
    "    return np.prod(probs[pos_ind])*np.prod(1-probs[neg_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed305d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingr = predict_vector(img_path, class_labels=class_labels_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133a396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bc31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0885f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4ffb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc173500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2992e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1a46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25ffbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
